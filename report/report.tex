% Template from the DPHPC course website

% IEEE standard conference template; to be used with:
%   spconf.sty  - LaTeX style file, and
%   IEEEbib.bst - IEEE bibliography style file.
% --------------------------------------------------------------------------

\documentclass[letterpaper]{article}
\usepackage{spconf,amsmath,amssymb,graphicx}
\usepackage[english]{babel}
\usepackage[utf8]{inputenc}
\usepackage{verbatim}		
\usepackage{epstopdf}
\usepackage[font=footnotesize]{caption}
\usepackage[dvipsnames]{xcolor}
\usepackage{hyperref}
\usepackage{listings}	
\usepackage{multicol}
\usepackage{ifthen}

\usepackage[ruled,noend,noline,slide]{algorithm2e}


% Example definitions.
% --------------------
% Nice typeset for C++
\newcommand{\Cpp}{C\nolinebreak[4]\hspace{-.05em}\raisebox{.2ex}{\small \bf ++}}
%\newcommand{\Cpp}{C\nolinebreak\hspace{-.05em}\raisebox{.4ex}{\tiny\bf +}\nolinebreak\hspace{-.10em}\raisebox{.4ex}{\tiny\bf +}}
%\newcommand{\Cpp}{\texttt{C\nolinebreak+\nolinebreak+}}


% bold paragraph titles
\newcommand{\mypar}[1]{{\bf #1.}}

% For example:
% ------------
%\address{School\\
%		 Department\\
%		 Address}
%
% Two addresses (uncomment and modify for two-address case).
% ----------------------------------------------------------
%\twoauthors
%  {A. Author-one, B. Author-two\sthanks{Thanks to XYZ agency for funding.}}
%		 {School A-B\\
%		 Department A-B\\
%		 Address A-B}
%  {C. Author-three, D. Author-four\sthanks{The fourth author performed the work
%		 while at ...}}
%		 {School C-D\\
%		 Department C-D\\
%		 Address C-D}
%


\ifthenelse{\boolean{true}}% TODO: change to false to hide invisible notes
{ \newenvironment{invisible}{\par\bigskip\color{gray}}{\par\bigskip} }%
{ \newenvironment{invisible}{\expandafter\comment}{\expandafter\endcomment} }

\title{Topological Sorting: A Parallel Implementation}

\name{J. Baum, K. Wallimann, M. Untergassmair}%
\address{ETH Zürich, HS 2015 \\
	Design of Parallel and High Performance Computing \\
	Zürich, Switzerland}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


	
	
	
\begin{document}

\maketitle

\begin{abstract}
 We can already start putting some theory parts in the report right now, so it's easier later to get everything together\ldots \\
Formatting comes later.
\end{abstract}


\begin{invisible}
	This will not be in final report \\
	we can put derivations and remarks here
\end{invisible}

\section{Introduction}\label{sec:intro}

\mypar{Motivation}
\begin{invisible}
  \begin{itemize}
  \item Software Dependencies
  \item Maybe, to flesh out: Admittedly a bit academic, but interesting problem nevertheless, because memory bound => This is the future of HPC
  \end{itemize}
\end{invisible}


\mypar{Related work} 
\begin{invisible}
  \begin{itemize}
  \item MC Er Paper \cite{er1983parallel}: Unclear how to retrieve a sorted list from values without sorting and threads might chase other threads. No words about load balancing => Not practicable
  \item Ma Paper \cite{ma1997efficient}: Theoretical analysis in PRAM model, not practicable.
  \item Both cases: No code
  \item Our contribution: (1) Modified algorithm based on MC Er. 1. Sorted list is directly extracted. 2. only one thread continues when multiple threads meet. 3. Ensure load balancing
                          (2) Actual implementation for shared memory architecture
  \end{itemize}
\end{invisible}


\section{Background: Whatever the Background is}\label{sec:background}
%TODO: Johannes

 In this section, we define the topological sort problem and contrast it to BFS and DFS.
 Furthermore, we introduce the basics of the parallel algorithms we use (and a cost analysis?).
 
 \begin{invisible}
 % Serial TS
 \mypar{Topological sorting}
 \begin{itemize}
  \item What is topological sort, difference to BFS
  \item Input: A set of dependencies (aka partial orders) of the form A $\rightarrow$ B ``A must come before B''
  \item Output: A sequence (aka total order) containing all nodes exactly once. All partial orders must be kept.
  \item Solution not unique
  \item Minimal Example: A->B, A->C, B->C. Valid BFS traversal order: A, C, B. Invalid for TS.
  \item TS can (serially) be solved with Kahn's algorithm \cite{kahn1962topological} or DFS and Backpropagation (Tarjan \cite{tarjan1976edge}). % See Wikipedia
        Note that TS is not equivalent to DFS, e.g. for A->B, B->C, A->D, D->E, DFS and Backpropagation yields A, B, C, D, E, but another valid TS is A, B, D, C, E
  \item Asymptotic runtime: O(|V| + |E|)
As an aside, don't talk about "the complexity of the algorithm.'' It's incorrect,
problems have a complexity, not algorithms.  
 \end{itemize}

 % Multithreaded TS
 \mypar{Parallel algorithm}
 \begin{itemize}
  \item Short overview over algorithm of MC Er
  \item Parallelization over child nodes
  \item His idea with barrier in each step such that even if the index is written by multiple threads, they write the same number => Avoid race condition at writing the index
  \item Our idea: Instead of writing an index, directly write to solution list. As a consequence, we have to make sure that node is written to solution only once. And of course there is a race condition on writing to solution list.
  \item Our idea: First, count (in parallel) how many parents each node has. Each time a node is visited, decrement counter and only write to solution if counter is zero. Of course, there is a race condition on the parent counter.
  \item 3 synchronization points (that is, bottlenecks): 1. Barrier after each level, 2. Lock solution list for appending new nodes, 3. Lock parent counter for decrementing it and checking if it is zero.
  \item Cost
 \end{itemize}

 
\end{invisible}

%TODO: Change title
\section{Efficient parallel implementation}\label{sec:yourmethod}
%TODO: Kevin
%TODO: Somewhere, we must state the problem: Input many partial orderings, Output one total order (a solution list)

In this section, we present different implementations of the parallel algorithm outlined above.
Especially, we show how to ensure load balancing, how to efficiently handle appending nodes to the solution list, decrementing and checking the parent counter, and how to circumvent barriers.

\mypar{Load balancing}
Parallelization is achieved by distributing the nodes in the current front. We experimented with two representations of the front.
Firstly, we implemented the front using thread-local lists, following an idea described in \cite{bulucc2011parallel}. Initially, the nodes in the front are split among the threads.
Each thread owns then a thread-local list, from which it processes the nodes in the current front. Child nodes for the next front are first inserted to another thread-local list.
When the whole front was processed, one thread collects all thread-local lists and redistributes the new child nodes among the threads.

Secondly, we implemented the front using a bitset, like in \cite{agarwal2010scalable} or \cite{beamer2013direction}. The size of the bitset is equal to the number of nodes.
Notice that we actually refer to an array of booleans when using the term bitset. A space-efficient bitset is not thread-safe.
The last parent visiting a child node sets the child nodes' bit to true. Parallelization is then enabled by parallelizing the loop over the bitset.
Load balancing is conveniently achieved using a dynamic scheduler.


\mypar{Efficiently appending to the solution list}
The algorithm proceeds level by level.
All nodes in the current level have already been visited by all their parent nodes. % This is actually an invariant, but never mind.
This is ensured by admitting only those child nodes to the next level, for which the current node is the last visiting parent node. % Maybe belongs to algorithm description
The nodes in the current level are ready to be inserted to the global solution list.
Since all nodes in the current level have been visited by all parent nodes, one node cannot be a parent of another node in the same level.
Therefore, the order among the nodes on one level does not matter for the topological sorting. As a consequence, the nodes can be appended to the solution in parallel.
To that end, the solution list has to be locked for every appending of a node, which is not optimal.

The optimization that is proposed here is simple: Every thread inserts the nodes in a thread-local list and then appends the whole local list to the solution list.
On each level, the solution list has to be locked for each thread only once and not for every node once. For lists, appending another list can be done in constant time.

This optimization is based on the level-by-level processing of the nodes and on the fact that the solution list actually is a list and not any other container, e.g. vector.

\mypar{Efficiently decrementing and checking the parent counter}
In the parallel algorithm, every node has a parent counter that is initialized with the number of parent nodes.
A node may only be inserted if all its parents have visited it.
Therefore, each parent has to decrement the parent counter atomically to mark its visit and it has to check atomically whether the parent counter is zero, i.e. whether it is the last visiting parent.

\begin{algorithm}
 \SetKw{KwRet}{Return}
 \SetKw{KwBool}{Bool}
 \SetKw{KwInt}{Integer}
 \SetKwProg{Fn}{Function}{}{}
  \KwInt parentCounter\;
  \tcp{initialized with number of parent nodes}
  \KwBool taken = false\;
  \Fn{decrementAndCheckParentCounter{}} {
    AtomicDecrement(parentCounter)\;
    \KwBool swapped = false\;
    \If{parentCounter == 0}{
      swapped = AtomicCompareAndSwap(taken, false, true)\;
    }
    \KwRet swapped;
  }
 \caption{Efficiently decrementing and checking the parent counter using atomics.}
 \label{alg:parentCounter}
\end{algorithm}

Algorithm \ref{alg:parentCounter} shows the implementation using atomic operations. Multiple threads may in fact decrement the counter before the function returns.
However the atomic compare-and-swap ensures that only one thread returns true. This is important if the current level is implemented as a list.
In this case, the child node could be inserted twice, which is wrong.
If the current level is implemented as a bitset, the compare-and-swap is not necessary, because it makes no difference if multiple threads set the bit to true.

\mypar{Barrier-free implementation}
Er \cite{er1983parallel} proposed barriers only to avoid race conditions when updating the index values at each node.
Using barriers allowed him not to lock the index value, since any thread writing to it would write the same value.

As explained earlier, our implementation directly appends to a solution list, instead of writing an index value.
Since the list must be locked for appending anyway, the barriers would in principle not be necessary.
However, this is only true if every node is appended individually, because a thread might append a node, whose parent may not yet have been moved from another thread's local list to the solution list.
Hence, avoiding barriers seems to be a trade-off between the cost of barriers and the cost of locking the solution list for every node.

\begin{invisible}
Hmm... we could defer decrementing the parent counter and inserting the child node until the local list is appended to the solution list. That means, that we have to touch all nodes twice
Somewhere, we need to mention that we are working with an adjacency list.
\end{invisible}

\section{Experimental Results}\label{sec:exp}
%TODO: Matthias
In this section, we present our experimental setup and the impact of the different optimizations and implementations described in the previous section.

\begin{invisible}
 \mypar{Experimental setup}
 \Cpp  OpenMP
 (processor, frequency, maybe OS, maybe cache sizes)
 compiler, version, and flags used.
 %TODO: Should we play with optimization flags?

 \mypar{Benchmarks}
 For each item, mention graph type, number of nodes, node degree, optimizations from above
 \begin{itemize}
  \item Influence of barrier. Introduce multichain graph, Strong Scaling dynamic nobarrier opt2 MULTICHAIN100 vs bitset global opt1 MULTICHAIN100 Why Multichain100: Should be dominated by barrier%bitset_global is bitset without local solutions, because dynamic_nobarrier has no local solutions
  \item Influence of local solution. Strong Scaling bitset MULTICHAIN10000 vs bitset global MULTICHAIN10000 Why Multichain10000: Should be dominated by pushbacks to solution
  \item Influence of atomic counter check. Strong Scaling RANDOMLIN 100000 opt = true vs opt = false for worksteal and bitset
  \item Strong Scaling software graph \cite{musco2014generative}
  \item Strong Scaling random graph with different degrees
  \item Vertex Scaling plot random graph
 \end{itemize}

\mypar{Results}

Questions to each plot
\begin{itemize}
 \item What is the performance penalty of the barrier?
 \item How well performs the local solution compared to locking the 
 \item How does the atomic counter check scale compared to the locked version?
 \item How does the best version of our code scale on a real-world example?
 \item How does the best version of our code scale in a slightly artificial scenario?
\end{itemize}
\end{invisible}


\section{Conclusions}

\begin{invisible}
 \begin{itemize}
   \item Best thing would be to have no barriers and still local solution update
   \item Since this is not possible, it is better to accept barriers so as to benefit from local solution
   \item It is worth noting that performance highly depends on the structure of the graph.
 \end{itemize}
\end{invisible}

% References should be produced using the bibtex program from suitable
% BiBTeX files (here: bibl_conf). The IEEEbib.bst bibliography
% style file from IEEE produces unsorted bibliography list.
% -------------------------------------------------------------------------
\bibliographystyle{IEEEbib}
\bibliography{bibliography}



\end{document}
